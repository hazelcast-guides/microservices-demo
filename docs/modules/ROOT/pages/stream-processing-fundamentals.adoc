////
Make sure to rename this file to the name of your repository and add the filename to the README. This filename must not conflict with any existing tutorials.
////

// Describe the title of your article by replacing 'Tutorial template' with the page name you want to publish.

= Pipeline API Tutorial - The Machine Shop
// Add required variables
:page-layout: tutorial
:page-product: platform, cloud 
:page-categories: Stream Processing
:page-lang: sql 
:page-enterprise: 
:page-est-time: 30 mins 

:description: Build a Hazelcast Pipeline for monitoring and reacting to telemetry from a machine shop.

We will be building a Hazelcast Pipeline for monitoring and reacting to telemetry from a machine shop.

ACME operates thousands of machines, each reporting several datapoints per second. Measurements include things like bit temperature and RPM.Breakage is expensive, both in replacement parts and in downtime. We want to go beyond maintenance schedules and monitor operations in real time. If excessive bit temperatures are caught in time, breakage can be averted by immediately reducing the cutting speed.

Each machine has its own parameters for acceptable bit temperature, which are stored in a `machine_profiles` IMap.   Our Pipeline will do this by sending "green" / "orange" / "red" signals to the machines 
via the `machine_controls` IMap.  A schematic of the lab is shown below.

image:pipeline.png[schematic]

== Before You Begin
You will need the following installed on your system:

* https://www.oracle.com/java/technologies/downloads/[JDK 17 or later]
* https://maven.apache.org/download.cgi[Maven 3.8 or later]
* Your preferred Java IDE (We recommend https://www.jetbrains.com/idea/download/?source=google&medium=cpc&campaign=AMER_en_US-PST+MST_IDEA_Branded&term=intellij+idea&content=602143185985&gad=1&gclid=Cj0KCQiAr8eqBhD3ARIsAIe-buM14qxoPph8ClqF1e4IL-xsv9LLe9w35ts2Q7Pt1uoS6vqc-8K-Cm0aAv1oEALw_wcB&section=mac[IntelliJ IDEA])
* https://docs.hazelcast.com/clc/5.3.5/install-clc[Hazelcast Command Line Client]
* https://www.docker.com/products/docker-desktop/[Docker Desktop]

== Step 1: Validate the Lab Environment
.  Run the following commands to start your lab environment.
+
```shell
mvn install
docker compose up -d
docker compose ps
```
+
You should see  4 services up and running.  You may also see a 5th service called `refdata_loader` which exits after 
it has loaded data into the `machine_profiles` map. 
+
Each service is described briefly below.
+
[cols="1,1"]
|===
| *Service*
| *Description*  

| hz 
| A Hazelcast node

| mc              
| The Hazelcast Management Center. Accessible at http://localhost:8080.   

| event_generator 
| Emulates traffic from a set of machines. Writes to `machine_events`. Listens to `machine_controls`. 

| ui
| A Python program that listens to the `machine_events` map and displays temperature data graphically. Accessible at http://localhost:8050 
|===

. Access the UI at http://localhost:8050. Specify a Location and Block to see a live display of the current temperature of a subset of machines.  Valid locations are "San Antonio" and "Los Angeles". Valid blocks are "A" and "B". 

[NOTE]
Machines in block "A" are very likely to run hot. You should see something similar to the image below.

image:UI.png[UI]

== Step 2: Get to Know the Data

Data in Hazelcast clusters can be accessed via SQL.  For more details, see https://docs.hazelcast.com/hazelcast/latest/sql/get-started-sql

. Open up the management center (http://localhost:8080) and click on the "SQL Browser" button.
+
image:MC_SQL.png[MC]

. Run `select * from machine_profiles` in the SQL Browser.
+
You will see that for each machine there is a serial number, information about the location and the warning and critical temperature limits for that particular machine.
+
image:profiles.png[profiles]

. Run `select * from machine_events`
+
image:machine_events.png[events]
+
This is the actual data coming from the machines.  
+
[NOTE]
At any given time, only the latest event for each serial number 
appears in the map, however, the update events are all recorded into something called a `map journal` that can be used as input to a pipeline.  You can verify that the map is constantly being updated by navigating to the `machine_events` map in Management Center and observing the number of put operations.
+
image:puts_and_entries.png[Puts]

. In a terminal window, set up the Command Line Client to connect to the Hazelcast instance in Docker, then open a CLC connection.
+
```shell
clc config add Docker cluster.name=dev cluster.address=localhost:5701
clc -c Docker
```

. Run the same SQL queries as above.

[NOTE]
From this point forward, you can use either Management Center or CLC to run SQL queries and monitor jobs. Management Center will also display the DAG created for submitted jobs.

== Step 3: Learn About Data Formats

=== GenericRecord

It is a best practice to avoid using POJOs in Pipelines if that POJO will be stored in a Hazelcast map.  It can cause 
class loading problems.  

[NOTE] 
When using _Compact_ or _Portable_ serialization, if a POJO is accessed in server-side 
code, it will be be deserialized as a _GenericRecord_ if the POJO class is not 
on the server's class path. Accordingly, server side code must be written to handle either
a _GenericRecord_ or a POJO depending on whether the POJO class is present.

In this lab, you will be working with _MachineProfile_ objects and _MachineEvents_, both of which are defined in `common/src/main/java`.  However, those classes are not deployed with your job, so you will need to access them using GenericRecord.  An example of GenericRecord usage is shown below.

```java
GenericRecord machineEvent;
short bitTemperature = machineEvent.getInt16("bitTemp");
```
For more information on GenericRecord and accessing domain objects without domain classes see
https://docs.hazelcast.com/hazelcast/5.3/clusters/accessing-domain-objects.

=== Tuples

As data proceeds through your pipeline its shape changes.  For example, you may look up the warning 
and critical temperatures for a particular machine and send them along with the original event to the next stage
in the pipeline.  There is no need to create special container classes for situations like this, you can use Tuples 
instead.  Here is an example.

```java
// create a 3-tuple that consists of the serial number and bit temperature from the event 
// and the warning temperature from the machine profile

GenericRecord p;
GenericRecord e;

Tuple3<String,Short,Short> newEvent = 
        Tuple3.tuple3(e.getString("serialNum"), e.getShort("bitTemp"), p.getShort("warningTemp"));

// now, if we want to access fields from the 3-tuple, we use f0(), f1() and f2()
short bitTemp = newEvent.f1();
```

== Step 4: Deploy Your First Job

. In your IDE, navigate to the *monitoring-pipeline* project.  Open up  the *hazelcast.platform.labs.machineshop.TemperatureMonitorPipeline* class and review the code there.  
+
The main method, shown below, is boilerplate that helps with deploying the job to a cluster. You do not need to change this.
+
```java
    public static void main(String []args){
        Pipeline pipeline = createPipeline();
        pipeline.setPreserveOrder(true);

        JobConfig jobConfig = new JobConfig();
        jobConfig.setName("Temperature Monitor");
        HazelcastInstance hz = Hazelcast.bootstrappedInstance();
        hz.getJet().newJob(pipeline, jobConfig);
    }
```
+
You will do all of your work in the `createPipeline` method of this job. It always starts with creating a `Pipeline` object.  You then build up the Pipeline by adding stages to it.
+
```java
   public static Pipeline createPipeline(){
        Pipeline pipeline = Pipeline.create();
        // add your stages here
        return pipeline;
   }
```
+
[NOTE] 

We use the Shade plugin to bundle all project dependencies, other than Hazelcast, into a single jar. The Hazelcast classes should not be included because they are already on the server. Code with `com.hazelcast` package names cannot be deployed to a Viridian cluster.
+
Currently, the `createPipeline` method contains only a source (reading from the `machine_events` map) and a sink, which simply logs the events to the console.  This can be useful during debugging. In the next step, you'll make a small change to the Pipeline and walk through a typical code/test cycle.

. Make a small change to the output format in the *writeTo* statement just so we can walk through building and deploying a pipeline.  After you've made the change, you can deploy the pipeline using the commands below.
+
```shell
cd monitoring-pipeline
mvn package
cd ..
# use the submission script, passing the name of the cluster you want to run the job
./clc-scripts/submitjob.sh Docker
# look for the logging statements in the Hazelcast logs
docker compose logs --follow hz
Ctrl-C
```
+
You should see something like this in the Hazelcast cluster member log:
```shell
stream-processing-fundamentals-hz-1  | 2023-02-01 21:11:44,357 [ INFO] [hz.hungry_lehmann.jet.blocking.thread-0] [c.h.j.i.c.WriteLoggerP]: [172.19.0.5]:5701 [dev] [5.2.1] [temp_monitor_161114/loggerSink#0] New Event SN=HYV569
stream-processing-fundamentals-hz-1  | 2023-02-01 21:11:44,370 [ INFO] [hz.hungry_lehmann.jet.blocking.thread-0] [c.h.j.i.c.WriteLoggerP]: [172.19.0.5]:5701 [dev] [5.2.1] [temp_monitor_161114/loggerSink#0] New Event SN=FXQ058
stream-processing-fundamentals-hz-1  | 2023-02-01 21:11:44,591 [ INFO] [hz.hungry_lehmann.jet.blocking.thread-0] [c.h.j.i.c.WriteLoggerP]: [172.19.0.5]:5701 [dev] [5.2.1] [temp_monitor_161114/loggerSink#0] New Event SN=RUO239
stream-processing-fundamentals-hz-1  | 2023-02-01 21:11:44,640 [ INFO] [hz.hungry_lehmann.jet.blocking.thread-0] [c.h.j.i.c.WriteLoggerP]: [172.19.0.5]:5701 [dev] [5.2.1] [temp_monitor_161114/loggerSink#0] New Event SN=DYQ714
```

. Inspect the running job using Management Center or CLC

. Cancel the job. The Hazelcast cluster will remain up and events will continue to flow. 
```shell
./clc-scripts/canceljob.sh Docker
```
+
image:firstjob.png[first job]

. Pat yourself on the back! You've deployed your first pipeline.

== Step 5: Finish the Pipeline

Continue building the pipeline following the instructions in `TemperatureMonitorPipeline.java`
You may want to deploy and cancel the job multiple times while you are building the pipeline. When you are done, look at the UI.  You should be able to tell that your job is now controlling the machines.

image:jobdone.png[job done]

You can also see machine control events in the `event_generator` log.

```shell
docker compose logs --follow event_generator
```

NOTE: If at any point you get stuck, you can refer to the solution which you will find in the
*hazelcast.platform.labs.machineshop.solutions* package.

== Step 6: Deploy  to Viridian

In this step, you will deploy your temperature monitoring Pipeline to a Viridian cluster and connect the UI, refdata loader and event_generator to it as well.

. If you haven't already done so, navigate to https://viridian.hazelcast.com, create an account, and create a new "Production" cluster.  This will deploy a 3 node cluster.  

. After the cluster is deployed, go to the CLI section of the "Quick Connection Guide" as shown below. Follow steps 2  and 3 to set up CLC for your Viridian cluster. 
+
image:viridian-clc-config.png[viridian-clc-config]

. The previous step should have given output similar to the following:
+
```
rmay@HZLCST-MBP-42 stream-processing-fundamentals % clc -c pr-tgouvd9r
Hazelcast CLC v5.3.5 (c) 2023 Hazelcast Inc.

* Participate in our survey at: https://forms.gle/rPFywdQjvib1QCe49
* Type 'help' for help information. Prefix non-SQL commands with \

Configuration : /Users/me/.hazelcast/configs/pr-tgouvd9r/config.yaml
Log      INFO : /Users/me/.hazelcast/logs/2023-11-27.log
```
We will need to tell Docker where to find the configuration files for your cluster.  In the above output, the configuration files for this cluster are  in the `$HOME/.hazelcast/configs/pr-tgouvd9r` directory.
+
In the shell or Windows command line where you will run your Docker commands, set the `VIRIDIAN_SECRETS_DIR` environment variable to the configuration directory.  See below for an example.  Be sure to use your Viridian cluster name.
+
```
export VIRIDIAN_SECRETS_DIR=$HOME/.hazelcast/configs/pr-tgouvd9r
```
+
. Start the refdata_loader, event_generator and ui using the viridian.compose setup. This pulls the environment variables you just set and allows the local Docker instances to talk to your Viridian cluster. 
+
```shell
docker compose -f viridian.compose.yaml up -d
```

. View the logs.
+
```shell
docker compose -f viridian.compose.yaml logs --follow
```

. Use Management Center via the Viridian console to verify that your cluster is receiving traffic.  

. Submit your job using the same script as before.  This time you will need to pass in the name of your cluster.  Use `clc config list` to see a list of known clusters.  An example is shown below.
+
```shell
clc-scripts % clc config list
 Configuration Name
 Docker
 dea-test
 pr-tgouvd9r  << Suppose you want to use this one

./clc-scripts/sumbit-job.sh pr-tgouvd9r
```
. Verify that the job is running using Management Center via the Viridian console. Look under _Stream Processing > Jobs_.

. We'll use CLC to view the logs from the Viridian cluster. First, you'll need to set up the API key and secret strings. In the Viridian console, go to _Account > Developer_ and set the API key. Do not navigate away from this screen.

. In your terminal window, open the CLC command line, specifying the name of your cluster. An example of the command is below.
+
```shell
clc -c pr-tgouvd9r
```

. At the CLC prompt, log in to your cluster. Enter the API key and secret when prompted.
+
```shell
\viridian login
```

. At the CLC prompt, view the logs for your cluster. You will need to specify the name of your cluster. An example is shown below. The output will be similar to the output you saw at the end of Step 4 above.
+
```shell
\viridian stream-logs EdSrvs
```


== Congratulations!


[NOTE]
This project contains many useful helpers.  Please feel free to study it and use it as a template for your own projects.










